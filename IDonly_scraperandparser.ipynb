{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1…\n",
      "Fetching page 2…\n",
      "Fetching page 3…\n",
      "Fetching page 4…\n",
      "Fetching page 5…\n",
      "Fetching page 6…\n",
      "Fetching page 7…\n",
      "Fetching page 8…\n",
      "Fetching page 9…\n",
      "Fetching page 10…\n",
      "Fetching page 11…\n",
      "Fetching page 12…\n",
      "Fetching page 13…\n",
      "Fetching page 14…\n",
      "Fetching page 15…\n",
      "Fetching page 16…\n",
      "Fetching page 17…\n",
      "Fetching page 18…\n",
      "Fetching page 19…\n",
      "Fetching page 20…\n",
      "Fetching page 21…\n",
      "Fetching page 22…\n",
      "Fetching page 23…\n",
      "Fetching page 24…\n",
      "Fetching page 25…\n",
      "Fetching page 26…\n",
      "Fetching page 27…\n",
      "Fetching page 28…\n",
      "Fetching page 29…\n",
      "Fetching page 30…\n",
      "Fetching page 31…\n",
      "Fetching page 32…\n",
      "Fetching page 33…\n",
      "Fetching page 34…\n",
      "Fetching page 35…\n",
      "Fetching page 36…\n",
      "Fetching page 37…\n",
      "Fetching page 38…\n",
      "Fetching page 39…\n",
      "Fetching page 40…\n",
      "Fetching page 41…\n",
      "Fetching page 42…\n",
      "Fetching page 43…\n",
      "Fetching page 44…\n",
      "✅ Done! File saved to: c:\\Users\\alons\\OneDrive - Cornell University\\Cornell University\\Spring 2025\\CFEM Research Assistant\\Parser\\royalty_scraper\\royaltyexchange_pages.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up Edge options\n",
    "edge_options = EdgeOptions()\n",
    "edge_options.add_argument(\"headless\")  # Remove this line if you want to see the browser\n",
    "\n",
    "# Path to Edge WebDriver\n",
    "service = EdgeService(r\"C:\\Users\\alons\\Downloads\\Installers\\edgedriver_win64\\msedgedriver.exe\")\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Edge(service=service, options=edge_options)\n",
    "\n",
    "# Base URL template\n",
    "base_url = (\n",
    "    \"https://auctions.royaltyexchange.com/orderbook/past-deals/\"\n",
    "    \"?filter{state.in}=filled&filter{state.in}=pending&filter{state.in}=closed\"\n",
    "    \"&sort[]=-deal_date&sort[]=-published_date&page_size=50\"\n",
    ")\n",
    "\n",
    "# Load page 1 to detect number of pages\n",
    "driver.get(f\"{base_url}&page=1\")\n",
    "time.sleep(2)\n",
    "buttons = driver.find_elements(By.CSS_SELECTOR, \"ul.MuiPagination-ul > li button\")\n",
    "page_numbers = [int(btn.text) for btn in buttons if btn.text.isdigit()]\n",
    "last_page = max(page_numbers) if page_numbers else 1\n",
    "\n",
    "all_html = []\n",
    "for page_num in range(1, last_page + 1):\n",
    "    print(f\"Fetching page {page_num}…\")\n",
    "    url = f\"{base_url}&page={page_num}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    all_html.append(driver.page_source)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "output_path = os.path.join(os.getcwd(), \"royaltyexchange_pages.txt\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\" + (\"=\"*100) + \"\\n\\n\".join(all_html))\n",
    "\n",
    "print(f\"✅ Done! File saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to new_extracted_listing_ids_2025-04-22.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load content from file\n",
    "with open(\"royaltyexchange_pages.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    new_content = f.read()\n",
    "\n",
    "# Extract all numbers that follow \"ID:\"\n",
    "new_ids = re.findall(r'ID:\\s*(\\d+)', new_content)\n",
    "\n",
    "# Create a DataFrame\n",
    "df_new_ids = pd.DataFrame(new_ids, columns=['Listing ID'])\n",
    "\n",
    "# Generate date string\n",
    "today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Save to CSV with date\n",
    "filename = f\"new_extracted_listing_ids_{today_str}.csv\"\n",
    "df_new_ids.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter & Update Existing Universe\n",
    "This code ensures that only truly new listings get added to your master universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986 new listings → unique_new_listing_ids_2025-04-22.csv\n",
      "Appended 986 to asset_ids_total.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load existing universe of asset ids\n",
    "master_path = \"asset_ids_total.csv\"\n",
    "master_df   = pd.read_csv(master_path, dtype=str)\n",
    "master_ids  = set(master_df[\"asset_id\"])\n",
    "\n",
    "# load new‑extracted file\n",
    "today       = datetime.today().strftime('%Y-%m-%d')\n",
    "new_path    = f\"new_extracted_listing_ids_{today}.csv\"\n",
    "new_df      = pd.read_csv(new_path, dtype=str)\n",
    "\n",
    "# if your CSV uses a different header, e.g. \"Listing ID\", rename it:\n",
    "if \"Listing ID\" in new_df.columns:\n",
    "    new_df = new_df.rename(columns={\"Listing ID\": \"asset_id\"})\n",
    "\n",
    "new_ids = set(new_df[\"asset_id\"])\n",
    "\n",
    "# filter down to only the truly new ones. no duplicates will be added to the universe\n",
    "unique_ids = new_ids - master_ids\n",
    "unique_df = new_df[new_df[\"asset_id\"].isin(unique_ids)]\n",
    "unique_path = f\"unique_new_listing_ids_{today}.csv\"\n",
    "unique_df.to_csv(unique_path, index=False)\n",
    "print(f\"{len(unique_df)} new listings → {unique_path}\")\n",
    "\n",
    "# update universe list\n",
    "if not unique_df.empty:\n",
    "    updated = pd.concat([master_df, unique_df], ignore_index=True)\n",
    "    updated.to_csv(master_path, index=False)\n",
    "    print(f\"Appended {len(unique_df)} to {master_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
