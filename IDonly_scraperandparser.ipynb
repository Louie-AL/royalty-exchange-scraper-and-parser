{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1…\n",
      "Fetching page 2…\n",
      "Fetching page 3…\n",
      "Fetching page 4…\n",
      "Fetching page 5…\n",
      "Fetching page 6…\n",
      "Fetching page 7…\n",
      "Fetching page 8…\n",
      "Fetching page 9…\n",
      "Fetching page 10…\n",
      "Fetching page 11…\n",
      "Fetching page 12…\n",
      "Fetching page 13…\n",
      "Fetching page 14…\n",
      "Fetching page 15…\n",
      "Fetching page 16…\n",
      "Fetching page 17…\n",
      "Fetching page 18…\n",
      "Fetching page 19…\n",
      "Fetching page 20…\n",
      "Fetching page 21…\n",
      "Fetching page 22…\n",
      "Fetching page 23…\n",
      "Fetching page 24…\n",
      "Fetching page 25…\n",
      "Fetching page 26…\n",
      "Fetching page 27…\n",
      "Fetching page 28…\n",
      "Fetching page 29…\n",
      "Fetching page 30…\n",
      "Fetching page 31…\n",
      "Fetching page 32…\n",
      "Fetching page 33…\n",
      "Fetching page 34…\n",
      "Fetching page 35…\n",
      "Fetching page 36…\n",
      "Fetching page 37…\n",
      "Fetching page 38…\n",
      "Fetching page 39…\n",
      "Fetching page 40…\n",
      "Fetching page 41…\n",
      "Fetching page 42…\n",
      "Fetching page 43…\n",
      "Fetching page 44…\n",
      "✅ Done! File saved to: c:\\Users\\alons\\OneDrive - Cornell University\\Cornell University\\Spring 2025\\CFEM Research Assistant\\Parser\\royalty_scraper\\royaltyexchange_pages.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up Edge options\n",
    "edge_options = EdgeOptions()\n",
    "edge_options.add_argument(\"headless\")  # Remove this line if you want to see the browser\n",
    "\n",
    "# Path to Edge WebDriver\n",
    "service = EdgeService(r\"C:\\Users\\alons\\Downloads\\Installers\\edgedriver_win64\\msedgedriver.exe\")\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Edge(service=service, options=edge_options)\n",
    "\n",
    "# Base URL template\n",
    "base_url = (\n",
    "    \"https://auctions.royaltyexchange.com/orderbook/past-deals/\"\n",
    "    \"?filter{state.in}=filled&filter{state.in}=pending&filter{state.in}=closed\"\n",
    "    \"&sort[]=-deal_date&sort[]=-published_date&page_size=50\"\n",
    ")\n",
    "\n",
    "# Load page 1 to detect number of pages\n",
    "driver.get(f\"{base_url}&page=1\")\n",
    "time.sleep(2)\n",
    "\n",
    "buttons = driver.find_elements(By.CSS_SELECTOR, \"ul.MuiPagination-ul > li button\")\n",
    "page_numbers = [int(btn.text) for btn in buttons if btn.text.isdigit()]\n",
    "last_page = max(page_numbers) if page_numbers else 1\n",
    "\n",
    "all_html = []\n",
    "for page_num in range(1, last_page + 1):\n",
    "    print(f\"Fetching page {page_num}…\")\n",
    "    url = f\"{base_url}&page={page_num}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    all_html.append(driver.page_source)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "output_path = os.path.join(os.getcwd(), \"royaltyexchange_pages.txt\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\" + (\"=\"*100) + \"\\n\\n\".join(all_html))\n",
    "\n",
    "print(f\"✅ Done! File saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to new_extracted_listing_ids_2025-04-22.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load content from file\n",
    "with open(\"royaltyexchange_pages.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    new_content = f.read()\n",
    "\n",
    "# Extract all numbers that follow \"ID:\"\n",
    "new_ids = re.findall(r'ID:\\s*(\\d+)', new_content)\n",
    "\n",
    "# Create a DataFrame\n",
    "df_new_ids = pd.DataFrame(new_ids, columns=['Listing ID'])\n",
    "\n",
    "# Generate date string\n",
    "today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Save to CSV with date\n",
    "filename = f\"new_extracted_listing_ids_{today_str}.csv\"\n",
    "df_new_ids.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Saved to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
